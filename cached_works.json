{
  "Application of linear algebra on ML": [
    {
      "paperId": "db9d6e92ec04dafe9a14fd0fc57cbd1d25cf6806",
      "url": "https://www.semanticscholar.org/paper/db9d6e92ec04dafe9a14fd0fc57cbd1d25cf6806",
      "title": "Amber: A 16-nm System-on-Chip With a Coarse- Grained Reconfigurable Array for Flexible Acceleration of Dense Linear Algebra",
      "venue": "IEEE Journal of Solid-State Circuits",
      "year": 2024,
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSSC.2023.3313116?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSSC.2023.3313116, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1994534921",
          "name": "Kathleen Feng"
        },
        {
          "authorId": "145381166",
          "name": "Taeyoung Kong"
        },
        {
          "authorId": "37877407",
          "name": "Kalhan Koul"
        },
        {
          "authorId": "50204540",
          "name": "Jackson Melchert"
        },
        {
          "authorId": "1752888567",
          "name": "Alex Carsello"
        },
        {
          "authorId": "3016287",
          "name": "Qiaoyi Liu"
        },
        {
          "authorId": "3414682",
          "name": "Gedeon Nyengele"
        },
        {
          "authorId": "26174584",
          "name": "Maxwell Strange"
        },
        {
          "authorId": "152645158",
          "name": "Kecheng Zhang"
        },
        {
          "authorId": "1563944172",
          "name": "Ankita Nayak"
        },
        {
          "authorId": "5291030",
          "name": "Jeff Setter"
        },
        {
          "authorId": "2152780507",
          "name": "James J. Thomas"
        },
        {
          "authorId": "151081519",
          "name": "Kavya Sreedhar"
        },
        {
          "authorId": "2143057145",
          "name": "Po-Han Chen"
        },
        {
          "authorId": "3422215",
          "name": "Nikhil Bhagdikar"
        },
        {
          "authorId": "2089917419",
          "name": "Zachary Myers"
        },
        {
          "authorId": "1742314030",
          "name": "Brandon D'Agostino"
        },
        {
          "authorId": "2004964411",
          "name": "Pranil Joshi"
        },
        {
          "authorId": "145326337",
          "name": "Stephen Richardson"
        },
        {
          "authorId": "2136471",
          "name": "Christopher Torng"
        },
        {
          "authorId": "2239739314",
          "name": "Mark Horowitz"
        },
        {
          "authorId": "50501653",
          "name": "Priyanka Raina"
        }
      ],
      "abstract": "Amber is a system-on-chip (SoC) with a coarse-grained reconfigurable array (CGRA) for acceleration of dense linear algebra applications, such as machine learning (ML), image processing, and computer vision. It is designed using an agile accelerator\u2013compiler codesign flow; the compiler updates automatically with hardware changes, enabling continuous application-level evaluation of the hardware\u2013software system. To increase hardware utilization and minimize reconfigurability overhead, Amber features the following: 1) dynamic partial reconfiguration (DPR) of the CGRA for higher resource utilization by allowing fast switching between applications and partitioning resources between simultaneous applications; 2) streaming memory controllers supporting affine access patterns for efficient mapping of dense linear algebra; and 3) low-overhead transcendental and complex arithmetic operations. The physical design of Amber features a unique clock distribution method and timing methodology to efficiently layout its hierarchical and tile-based design. Amber achieves a peak energy efficiency of 538 INT16 GOPS/W and 483 BFloat16 GFLOPS/W. Compared with a CPU, a GPU, and a field-programmable gate array (FPGA), Amber has up to 3902<inline-formula> <tex-math notation=\"LaTeX\">$\\times $ </tex-math></inline-formula>, 152<inline-formula> <tex-math notation=\"LaTeX\">$\\times $ </tex-math></inline-formula>, and 107<inline-formula> <tex-math notation=\"LaTeX\">$\\times $ </tex-math></inline-formula> better energy-delay product (EDP), respectively."
    },
    {
      "paperId": "a8aeb7ca7b593559d84f6948674cc4b188a2589f",
      "url": "https://www.semanticscholar.org/paper/a8aeb7ca7b593559d84f6948674cc4b188a2589f",
      "title": "A Simple and Practical Linear Algebra Library Interface with Static Size Checking",
      "venue": "ML/OCaml",
      "year": 2015,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1512.01898",
        "status": "GOLD",
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.01898, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "39596534",
          "name": "A. Abe"
        },
        {
          "authorId": "2391325",
          "name": "Eijiro Sumii"
        }
      ],
      "abstract": "Linear algebra is a major field of numerical computation and is widely applied. Most linear algebra libraries (in most programming languages) do not statically guarantee consistency of the dimensions of vectors and matrices, causing runtime errors. While advanced type systems--specifically, dependent types on natural numbers--can ensure consistency among the sizes of collections such as lists and arrays, such type systems generally require non-trivial changes to existing languages and application programs, or tricky type-level programming. \nWe have developed a linear algebra library interface that verifies the consistency (with respect to dimensions) of matrix operations by means of generative phantom types, implemented via fairly standard ML types and module system. To evaluate its usability, we ported to it a practical machine learning library from a traditional linear algebra library. We found that most of the changes required for the porting could be made mechanically, and changes that needed human thought are minor."
    },
    {
      "paperId": "192082193bb2f4e51437ef7b366fb5d64b3e800e",
      "url": "https://www.semanticscholar.org/paper/192082193bb2f4e51437ef7b366fb5d64b3e800e",
      "title": "Fundamental Bias in Inverting Random Sampling Matrices with Application to Sub-sampled Newton",
      "venue": "arXiv.org",
      "year": 2025,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.13583, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2091429810",
          "name": "Chengmei Niu"
        },
        {
          "authorId": "2282530885",
          "name": "Zhenyu Liao"
        },
        {
          "authorId": "2302328253",
          "name": "Zenan Ling"
        },
        {
          "authorId": "2346110373",
          "name": "Michael W. Mahoney"
        }
      ],
      "abstract": "A substantial body of work in machine learning (ML) and randomized numerical linear algebra (RandNLA) has exploited various sorts of random sketching methodologies, including random sampling and random projection, with much of the analysis using Johnson--Lindenstrauss and subspace embedding techniques. Recent studies have identified the issue of inversion bias -- the phenomenon that inverses of random sketches are not unbiased, despite the unbiasedness of the sketches themselves. This bias presents challenges for the use of random sketches in various ML pipelines, such as fast stochastic optimization, scalable statistical estimators, and distributed optimization. In the context of random projection, the inversion bias can be easily corrected for dense Gaussian projections (which are, however, too expensive for many applications). Recent work has shown how the inversion bias can be corrected for sparse sub-gaussian projections. In this paper, we show how the inversion bias can be corrected for random sampling methods, both uniform and non-uniform leverage-based, as well as for structured random projections, including those based on the Hadamard transform. Using these results, we establish problem-independent local convergence rates for sub-sampled Newton methods."
    },
    {
      "paperId": "9e52830af11701c7b07473c0b083bc7d0529bff5",
      "url": "https://www.semanticscholar.org/paper/9e52830af11701c7b07473c0b083bc7d0529bff5",
      "title": "Machine learning based method of moments (ML-MoM)",
      "venue": "2017 IEEE International Symposium on Antennas and Propagation & USNC/URSI National Radio Science Meeting",
      "year": 2017,
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/APUSNCURSINRSM.2017.8072529?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/APUSNCURSINRSM.2017.8072529, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "20856704",
          "name": "H. Yao"
        },
        {
          "authorId": "47420500",
          "name": "L. J. Jiang"
        },
        {
          "authorId": "30785980",
          "name": "Y. Qin"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "3102cdd23bf1c6f3a8d0b172e82e708b2482bad3",
      "url": "https://www.semanticscholar.org/paper/3102cdd23bf1c6f3a8d0b172e82e708b2482bad3",
      "title": "Machine learning based MoM (ML-MoM) for parasitic capacitance extractions",
      "venue": "Electrical Design of Advanced Packaging and Systems Symposium",
      "year": 2016,
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EDAPS.2016.7893155?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EDAPS.2016.7893155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "20856704",
          "name": "H. Yao"
        },
        {
          "authorId": "30785980",
          "name": "Y. Qin"
        },
        {
          "authorId": "47420500",
          "name": "L. J. Jiang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "d79ae631d13708887e4f57698eb4f54c0c54dbbc",
      "url": "https://www.semanticscholar.org/paper/d79ae631d13708887e4f57698eb4f54c0c54dbbc",
      "title": "Deep Learning and Machine Learning - Python Data Structures and Mathematics Fundamental: From Theory to Practice",
      "venue": "arXiv.org",
      "year": 2024,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.19849, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2111636693",
          "name": "Silin Chen"
        },
        {
          "authorId": "2319608188",
          "name": "Ziqian Bi"
        },
        {
          "authorId": "2319615017",
          "name": "Junyu Liu"
        },
        {
          "authorId": "2319606006",
          "name": "Benji Peng"
        },
        {
          "authorId": "2322292279",
          "name": "Sen Zhang"
        },
        {
          "authorId": "2323322733",
          "name": "Xuanhe Pan"
        },
        {
          "authorId": "2237064214",
          "name": "Jiawei Xu"
        },
        {
          "authorId": "2322363547",
          "name": "Jinlang Wang"
        },
        {
          "authorId": "2319648854",
          "name": "Keyu Chen"
        },
        {
          "authorId": "2323530189",
          "name": "Caitlyn Heqi Yin"
        },
        {
          "authorId": "2319607771",
          "name": "Pohsun Feng"
        },
        {
          "authorId": "2324088996",
          "name": "Yizhu Wen"
        },
        {
          "authorId": "2323695882",
          "name": "Tianyang Wang"
        },
        {
          "authorId": "2321058649",
          "name": "Ming Li"
        },
        {
          "authorId": "2327699288",
          "name": "Jintao Ren"
        },
        {
          "authorId": "2319611972",
          "name": "Qian Niu"
        },
        {
          "authorId": "2322495607",
          "name": "Ming Liu"
        }
      ],
      "abstract": "This book provides a comprehensive introduction to the foundational concepts of machine learning (ML) and deep learning (DL). It bridges the gap between theoretical mathematics and practical application, focusing on Python as the primary programming language for implementing key algorithms and data structures. The book covers a wide range of topics, including basic and advanced Python programming, fundamental mathematical operations, matrix operations, linear algebra, and optimization techniques crucial for training ML and DL models. Advanced subjects like neural networks, optimization algorithms, and frequency domain methods are also explored, along with real-world applications of large language models (LLMs) and artificial intelligence (AI) in big data management. Designed for both beginners and advanced learners, the book emphasizes the critical role of mathematical principles in developing scalable AI solutions. Practical examples and Python code are provided throughout, ensuring readers gain hands-on experience in applying theoretical knowledge to solve complex problems in ML, DL, and big data analytics."
    },
    {
      "paperId": "cd79eaf171c235bf9351f6ed1f0599a4de37e1b9",
      "url": "https://www.semanticscholar.org/paper/cd79eaf171c235bf9351f6ed1f0599a4de37e1b9",
      "title": "Mobile Learning Improves Student Learning Outcomes in Mathematics Education",
      "venue": "Journal for Lesson and Learning Studies",
      "year": 2023,
      "openAccessPdf": {
        "url": "https://ejournal.undiksha.ac.id/index.php/JLLS/article/download/61417/25964",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.23887/jlls.v6i1.61417?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.23887/jlls.v6i1.61417, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2219254568",
          "name": "Dori Lukman Hakim"
        }
      ],
      "abstract": "This study discusses how the resulting influence on student learning outcomes using Mobile Learning (ML). The research was conducted at the Bachelor of Mathematics Education Study Program, Singaperbangsa University, Karawang in semester III of 96 students from three classes which constituted the population as well as the sample in this study as a whole. This study examines how the influence of the application of Mobile Learning on student learning outcomes, which will be seen based on student ability groups (High Category, Medium Category, Low Category), then based on each class (Class A, Class B, Class C), and overall students. The method used in this study is quantitative with the One Sample Group Posttest Design, with learning outcomes taken from the final grades of the learning process by applying mobile learning to the Linear Algebra Course. The results of this research are as follows; 1). The effect of Mobile Learning (ML) on the student learning outcomes group for the high category did not give any effect, but the student learning outcomes group for the medium and low categories did have an influence. 2). The influence of Mobile Learning (ML) on student learning outcomes in each class, namely class A, class B, and class C, shows a significant influence, so it is good to be given to any class. 3). The influence of Mobile Learning (ML) on student learning outcomes as a whole shows a significant influence"
    },
    {
      "paperId": "98ecc1122bbf73b280ff6e49d54377de1cfbbb64",
      "url": "https://www.semanticscholar.org/paper/98ecc1122bbf73b280ff6e49d54377de1cfbbb64",
      "title": "The Role of Mathematics in Artificial Intelligence and Machine Learning",
      "venue": "International Journal for Research Publication and Seminar",
      "year": 2023,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.36676/jrps.v14.i5.1434?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.36676/jrps.v14.i5.1434, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2312690090",
          "name": "Kuldeep Singh"
        }
      ],
      "abstract": "Mathematics serves as the foundational backbone of \u201cartificial intelligence (AI) and machine learning (ML), providing the essential\u201d tools and frameworks for developing sophisticated algorithms and models. the pivotal role of various mathematical disciplines, including linear algebra, calculus, probability theory, and optimization, in advancing AI and ML technologies. We begin by examining how linear algebra facilitates the manipulation and transformation of high-dimensional data, which is crucial for \u201ctechniques such as principal component analysis (PCA) and singular value decomposition (SVD)\u201d. Next, we delve into the applications of calculus in training neural networks through gradient-based optimization methods, highlighting the importance of differentiation and integration in backpropagation and loss function minimization. the role of probability theory in handling uncertainty and making predictions, emphasizing its application in Bayesian networks, Markov decision processes, and probabilistic graphical models. Additionally, we discuss optimization techniques, both convex and non-convex, that are fundamental to finding optimal solutions in machine learning tasks, including support vector machines (SVMs) and deep learning architectures."
    },
    {
      "paperId": "270181e363bad86660769e70280d4eaa312fa987",
      "url": "https://www.semanticscholar.org/paper/270181e363bad86660769e70280d4eaa312fa987",
      "title": "Kernel charge equilibration: efficient and accurate prediction of molecular dipole moments with a machine-learning enhanced electron density model",
      "venue": "Machine Learning: Science and Technology",
      "year": 2021,
      "openAccessPdf": {
        "url": "https://doi.org/10.1088/2632-2153/ac568d",
        "status": "GOLD",
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1088/2632-2153/ac568d?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1088/2632-2153/ac568d, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1962728519",
          "name": "Carsten G. Staacke"
        },
        {
          "authorId": "30010032",
          "name": "Simon Wengert"
        },
        {
          "authorId": "81286843",
          "name": "C. Kunkel"
        },
        {
          "authorId": "2559761",
          "name": "G\u00e1bor Cs\u00e1nyi"
        },
        {
          "authorId": "144148678",
          "name": "K. Reuter"
        },
        {
          "authorId": "39203264",
          "name": "Johannes T. Margraf"
        }
      ],
      "abstract": "State-of-the-art machine learning (ML) interatomic potentials use local representations of atomic environments to ensure linear scaling and size-extensivity. This implies a neglect of long-range interactions, most prominently related to electrostatics. To overcome this limitation, we herein present a ML framework for predicting charge distributions and their interactions termed kernel charge equilibration (kQEq). This model is based on classical charge equilibration (QEq) models expanded with an environment-dependent electronegativity. In contrast to previously reported neural network models with a similar concept, kQEq takes advantage of the linearity of both QEq and Kernel Ridge Regression to obtain a closed-form linear algebra expression for training the models. Furthermore, we avoid the ambiguity of charge partitioning schemes by using dipole moments as reference data. As a first application, we show that kQEq can be used to generate accurate and highly data-efficient models for molecular dipole moments."
    },
    {
      "paperId": "c1ba3bd10f9270e8e19a1af1de2b2f18965856b8",
      "url": "https://www.semanticscholar.org/paper/c1ba3bd10f9270e8e19a1af1de2b2f18965856b8",
      "title": "An Information-Theoretic Approach to Personalized Explainable Machine Learning",
      "venue": "IEEE Signal Processing Letters",
      "year": 2020,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2003.00484",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.00484, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "145438916",
          "name": "A. Jung"
        },
        {
          "authorId": "145846313",
          "name": "P. Nardelli"
        }
      ],
      "abstract": "Automated decision making is used routinely throughout our every-day life. Recommender systems decide which jobs, movies, or other user profiles might be interesting to us. Spell checkers help us to make good use of language. Fraud detection systems decide if a credit card transactions should be verified more closely. Many of these decision making systems use machine learning methods that fit complex models to massive datasets. The successful deployment of machine learning (ML) methods to many (critical) application domains crucially depends on its explainability. Indeed, humans have a strong desire to get explanations that resolve the uncertainty about experienced phenomena like the predictions and decisions obtained from ML methods. Explainable ML is challenging since explanations must be tailored (personalized) to individual users with varying backgrounds. Some users might have received university-level education in ML, while other users might have no formal training in linear algebra. Linear regression with few features might be perfectly interpretable for the first group but might be considered a black-box by the latter. We propose a simple probabilistic model for the predictions and user knowledge. This model allows to study explainable ML using information theory. Explaining is here considered as the task of reducing the \u201csurprise\u201d incurred by a prediction. We quantify the effect of an explanation by the conditional mutual information between the explanation and prediction, given the user background."
    },
    {
      "paperId": "aadcdb9d4d8c376720a29da2a3e4403bac405bc6",
      "url": "https://www.semanticscholar.org/paper/aadcdb9d4d8c376720a29da2a3e4403bac405bc6",
      "title": "An Information-Theoretic Approach to Explainable Machine Learning",
      "venue": "arXiv.org",
      "year": 2020,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "145438916",
          "name": "A. Jung"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "bd8a4080f4c28e8bb2ef93ab5a04ec8d77ab140e",
      "url": "https://www.semanticscholar.org/paper/bd8a4080f4c28e8bb2ef93ab5a04ec8d77ab140e",
      "title": "Python Libraries, Development Frameworks and Algorithms for Machine Learning Applications",
      "venue": "",
      "year": 2018,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "2288750955",
          "name": "Dr. V. Hanuman Kumar"
        },
        {
          "authorId": "2288504132",
          "name": "Sr. Assoc. Prof"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "4186cd4aa8f6164284c76c8c7ee6f7b1c595583b",
      "url": "https://www.semanticscholar.org/paper/4186cd4aa8f6164284c76c8c7ee6f7b1c595583b",
      "title": "Synthesis Framework for Executing Neural Networks on Heterogeneous Platforms",
      "venue": "",
      "year": 2019,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "153642652",
          "name": "Syed Haider"
        },
        {
          "authorId": "144564537",
          "name": "K. Schneider"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "e05e887b1c9e1501ee9db52c521fa3b806d56bd5",
      "url": "https://www.semanticscholar.org/paper/e05e887b1c9e1501ee9db52c521fa3b806d56bd5",
      "title": "Optimizing Sparse Linear Algebra Through Automatic Format Selection and Machine Learning",
      "venue": "IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum",
      "year": 2023,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2303.05098",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.05098, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2149498429",
          "name": "Christodoulos Stylianou"
        },
        {
          "authorId": "92006026",
          "name": "M. Weiland"
        }
      ],
      "abstract": "Sparse matrices are an integral part of scientific simulations. As hardware evolves new sparse matrix storage formats are proposed aiming to exploit optimizations specific to the new hardware. In the era of heterogeneous computing, users often are required to use multiple formats for their applications to remain optimal across the different available hardware, resulting in larger development times and maintenance overhead. A potential solution to this problem is the use of a lightweight auto-tuner driven by Machine Learning (ML) that would select for the user an optimal format from a pool of available formats that will match the characteristics of the sparsity pattern, target hardware and operation to execute. In this paper, we introduce Morpheus-Oracle, a library that provides a lightweight ML auto-tuner capable of accurately predicting the optimal format across multiple backends, targeting the major HPC architectures aiming to eliminate any format selection input by the end-user. From more than 2000 real-life matrices, we achieve an average classification accuracy and balanced accuracy of 92.63% and $80. 22$% respectively across the available systems. The adoption of the auto-tuner results in average speedup of $1. 1 \\times $ on CPUs and $1. 5 \\times $ to $8 \\times $ on NVIDIA and AMD GPUs, with maximum speedups reaching up to $7 \\times $ and $1000 \\times $ respectively."
    },
    {
      "paperId": "843fcc0d9d60b8651029d8c45a97688c1b054e0b",
      "url": "https://www.semanticscholar.org/paper/843fcc0d9d60b8651029d8c45a97688c1b054e0b",
      "title": "Amber: A 367 GOPS, 538 GOPS/W 16nm SoC with a Coarse-Grained Reconfigurable Array for Flexible Acceleration of Dense Linear Algebra",
      "venue": "2022 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits)",
      "year": 2022,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/vlsitechnologyandcir46769.2022.9830509?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/vlsitechnologyandcir46769.2022.9830509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1752888567",
          "name": "Alex Carsello"
        },
        {
          "authorId": "1994534921",
          "name": "Kathleen Feng"
        },
        {
          "authorId": "145381166",
          "name": "Taeyoung Kong"
        },
        {
          "authorId": "37877407",
          "name": "Kalhan Koul"
        },
        {
          "authorId": "3016287",
          "name": "Qiaoyi Liu"
        },
        {
          "authorId": "50204540",
          "name": "Jackson Melchert"
        },
        {
          "authorId": "3414682",
          "name": "Gedeon Nyengele"
        },
        {
          "authorId": "26174584",
          "name": "Maxwell Strange"
        },
        {
          "authorId": "152645158",
          "name": "Kecheng Zhang"
        },
        {
          "authorId": "1563944172",
          "name": "Ankita Nayak"
        },
        {
          "authorId": "5291030",
          "name": "Jeff Setter"
        },
        {
          "authorId": "2152780507",
          "name": "James J. Thomas"
        },
        {
          "authorId": "151081519",
          "name": "Kavya Sreedhar"
        },
        {
          "authorId": "2143057145",
          "name": "Po-Han Chen"
        },
        {
          "authorId": "3422215",
          "name": "Nikhil Bhagdikar"
        },
        {
          "authorId": "2089917419",
          "name": "Zachary Myers"
        },
        {
          "authorId": "1742314030",
          "name": "Brandon D'Agostino"
        },
        {
          "authorId": "2004964411",
          "name": "Pranil Joshi"
        },
        {
          "authorId": "145326337",
          "name": "Stephen Richardson"
        },
        {
          "authorId": "1752888571",
          "name": "Rick Bahr"
        },
        {
          "authorId": "2136471",
          "name": "Christopher Torng"
        },
        {
          "authorId": "144764327",
          "name": "M. Horowitz"
        },
        {
          "authorId": "50501653",
          "name": "Priyanka Raina"
        }
      ],
      "abstract": "Amber is a system-on-chip (SoC) with a coarse-grained reconfigurable array (CGRA) for acceleration of dense linear algebra applications such as machine learning (ML), image processing, and computer vision. It achieves a peak energy efficiency of 538.0 INT16 GOPS/W and 483.3 BFloat16 GFLOPS/W. We maximize CGRA utilization and minimize reconfigurability overhead through (1) dynamic partial reconfiguration of the CGRA that enables higher resource utilization by allowing multiple applications to run at once, (2) efficient streaming memory controllers supporting affine access patterns, and (3) low-overhead transcendental and complex arithmetic operations. Compared to a CPU, a GPU, and an FPGA, Amber achieves up to 3902x, 152x, and 88x better energy-delay product (EDP)."
    },
    {
      "paperId": "0f2147441622683c23b530005163553632b2a541",
      "url": "https://www.semanticscholar.org/paper/0f2147441622683c23b530005163553632b2a541",
      "title": "A Universal, Analog, In-Memory Computing Primitive for Linear Algebra Using Memristors",
      "venue": "IEEE Transactions on Circuits and Systems Part 1: Regular Papers",
      "year": 2021,
      "openAccessPdf": {
        "url": "https://re.public.polimi.it/bitstream/11311/1189813/4/TCAS_LinReg_v2.pdf",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/tcsi.2021.3122278?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/tcsi.2021.3122278, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1689627617",
          "name": "P. Mannocci"
        },
        {
          "authorId": "47404896",
          "name": "G. Pedretti"
        },
        {
          "authorId": "120080425",
          "name": "E. Giannone"
        },
        {
          "authorId": "2137364331",
          "name": "E. Melacarne"
        },
        {
          "authorId": "2218486714",
          "name": "Zhong Sun"
        },
        {
          "authorId": "144983981",
          "name": "D. Ielmini"
        }
      ],
      "abstract": "The increasing demand for data-intensive computing applications, such as artificial intelligence (AI) and more specifically machine learning (ML), raises the need for novel computing hardware architectures capable of massive parallelism in performing core algebraic operations. Among the new paradigms, in-memory computing (IMC) with analogue devices is attracting significant interest for its large-scale integration potential, together with unrivaled speed and energy performance. Here, we present a fully-analogue, universal primitive capable of executing linear algebra operations such as regression, generalized least-square minimization and linear system solution with and without preconditioning. We study the impact of the main circuit parameters on accuracy and bandwidth with analytical closed-form expressions and SPICE simulations. Scaling challenges due to parasitic resistance/capacitance and their impact on key parameters such as bandwidth and accuracy are discussed. Finally, a comparison with existing solvers belonging to the same IMC framework is made to assess advantages and disadvantages of the proposed circuit."
    },
    {
      "paperId": "d0a7b73aac57f55c3dc9a81701c8c8b060733dbd",
      "url": "https://www.semanticscholar.org/paper/d0a7b73aac57f55c3dc9a81701c8c8b060733dbd",
      "title": "A Geometric Algebra Approach to Invariance Control in Sliding Regimes for Switched Systems",
      "venue": "Advances in Applied Clifford Algebras",
      "year": 2023,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00006-023-01281-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00006-023-01281-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2277469408",
          "name": "H. Sira-Ram\u00b4\u0131rez"
        },
        {
          "authorId": "2277473387",
          "name": "B. C. G\u00b4omez-Le\u00b4on"
        },
        {
          "authorId": "2277470537",
          "name": "M. A. Aguilar-Ordu\u02dcna"
        },
        {
          "authorId": "2237451392",
          "name": "Adv. Appl"
        },
        {
          "authorId": "2277470535",
          "name": "H. Cli\ufb00ordAlgebras"
        },
        {
          "authorId": "2277470015",
          "name": "A. Sira-Ram\u00b4\u0131rezM."
        },
        {
          "authorId": "2277473381",
          "name": "Aguilar-Ordu\u02dcna"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "5f26bfbf03bcacafe9099ae7e1aaeb4c88a88dad",
      "url": "https://www.semanticscholar.org/paper/5f26bfbf03bcacafe9099ae7e1aaeb4c88a88dad",
      "title": "Application of an Exploratory Knowledge-Discovery Pipeline Based on Machine Learning to Multi-Scale OMICS Data to Characterise Myocardial Injury in a Cohort of Patients with Septic Shock: An Observational Study",
      "venue": "Journal of Clinical Medicine",
      "year": 2021,
      "openAccessPdf": {
        "url": "https://www.mdpi.com/2077-0383/10/19/4354/pdf?version=1632479315",
        "status": "GOLD",
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8509561, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "48290141",
          "name": "B. Bollen Pinto"
        },
        {
          "authorId": "115757355",
          "name": "Vicent Ribas Ripoll"
        },
        {
          "authorId": "1419448102",
          "name": "Paula Sub\u00edas-Beltr\u00e1n"
        },
        {
          "authorId": "5138861",
          "name": "A. Herpain"
        },
        {
          "authorId": "4509045",
          "name": "C. Barlassina"
        },
        {
          "authorId": "46988726",
          "name": "E. Oliveira"
        },
        {
          "authorId": "2003494",
          "name": "R. Pastorelli"
        },
        {
          "authorId": "47144545",
          "name": "D. Braga"
        },
        {
          "authorId": "49419990",
          "name": "M. Barcella"
        },
        {
          "authorId": "3318559",
          "name": "L. Subirats"
        },
        {
          "authorId": "2269461636",
          "name": "J. Bauz\u00e1-Martinez"
        },
        {
          "authorId": "12457304",
          "name": "Antonia Odena"
        },
        {
          "authorId": "2166600",
          "name": "M. Ferrario"
        },
        {
          "authorId": "2912339",
          "name": "G. Baselli"
        },
        {
          "authorId": "2454693",
          "name": "F. Aletti"
        },
        {
          "authorId": "6445977",
          "name": "K. Bendjelid"
        },
        {
          "authorId": "2132403651",
          "name": "On Behalf Of The Shockomics Consortium"
        }
      ],
      "abstract": "Currently, there is no therapy targeting septic cardiomyopathy (SC), a key contributor to organ dysfunction in sepsis. In this study, we used a machine learning (ML) pipeline to explore transcriptomic, proteomic, and metabolomic data from patients with septic shock, and prospectively collected measurements of high-sensitive cardiac troponin and echocardiography. The purposes of the study were to suggest an exploratory methodology to identify and characterise the multiOMICs profile of (i) myocardial injury in patients with septic shock, and of (ii) cardiac dysfunction in patients with myocardial injury. The study included 27 adult patients admitted for septic shock. Peripheral blood samples for OMICS analysis and measurements of high-sensitive cardiac troponin T (hscTnT) were collected at two time points during the ICU stay. A ML-based study was designed and implemented to untangle the relations among the OMICS domains and the aforesaid biomarkers. The resulting ML pipeline consisted of two main experimental phases: recursive feature selection (FS) assessing the stability of biomarkers, and classification to characterise the multiOMICS profile of the target biomarkers. The application of a ML pipeline to circulate OMICS data in patients with septic shock has the potential to predict the risk of myocardial injury and the risk of cardiac dysfunction."
    },
    {
      "paperId": "6e7a7278a2322c6eac1376d8b6bb8166b998301d",
      "url": "https://www.semanticscholar.org/paper/6e7a7278a2322c6eac1376d8b6bb8166b998301d",
      "title": "AI-enhanced iterative solvers for accelerating the solution of large scale parametrized linear systems of equations",
      "venue": "arXiv.org",
      "year": 2022,
      "openAccessPdf": {
        "url": "https://zenodo.org/record/7296699/files/DCoMEX_publication.pdf",
        "status": "GREEN",
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2207.02543?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2207.02543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2046875710",
          "name": "Stefanos Nikolopoulos"
        },
        {
          "authorId": "97999246",
          "name": "I. Kalogeris"
        },
        {
          "authorId": "49630868",
          "name": "V. Papadopoulos"
        },
        {
          "authorId": "49591290",
          "name": "G. Stavroulakis"
        }
      ],
      "abstract": "Recent advances in the \ufb01eld of machine learning open a new era in high performance computing. Applications of machine learning algorithms for the development of accurate and cost-e\ufb03cient surrogates of complex problems have already attracted major attention from scientists. Despite their powerful approximation ca-pabilities, however, surrogates cannot produce the \u2018exact\u2019 solution to the problem. To address this issue, this paper exploits up-to-date ML tools and delivers customized iterative solvers of linear equation systems, capable of solving large-scale parametrized problems at any desired level of accuracy. Speci\ufb01cally, the proposed approach consists of the following two steps. At \ufb01rst, a reduced set of model evaluations is performed and the corresponding solutions are used to establish an approximate mapping from the problem\u2019s parametric space to its solution space using deep feedforward neural networks and convolutional autoencoders. This mapping serves a means to obtain very accurate initial predictions of the system\u2019s response to new query points at negligible computational cost. Subsequently, an iterative solver inspired by the Algebraic Multigrid method in combination with Proper Orthogonal Decomposition, termed POD-2G, is developed that successively re\ufb01nes the initial predictions towards the exact system solutions. The application of POD-2G as a standalone solver or as preconditioner in the context of preconditioned conjugate gradient methods is demonstrated on several numerical examples of large scale systems, with the results indicating its superiority over conventional iterative solution schemes."
    },
    {
      "paperId": "b92825d268f7c2098b45ae313be85c1610f1a2fb",
      "url": "https://www.semanticscholar.org/paper/b92825d268f7c2098b45ae313be85c1610f1a2fb",
      "title": "BB-ML: Basic Block Performance Prediction using Machine Learning Techniques",
      "venue": "International Conference on Parallel and Distributed Systems",
      "year": 2022,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2202.07798",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.07798, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "152377088",
          "name": "Shamminuj Aktar"
        },
        {
          "authorId": "2152445086",
          "name": "H. Abdelkhalik"
        },
        {
          "authorId": "1564593718",
          "name": "Nazmul Haque Turja"
        },
        {
          "authorId": "51109929",
          "name": "Yehia Arafa"
        },
        {
          "authorId": "36069251",
          "name": "Atanu Barai"
        },
        {
          "authorId": "39912591",
          "name": "N. Panda"
        },
        {
          "authorId": "2530185",
          "name": "Gopinath Chennupati"
        },
        {
          "authorId": "28968696",
          "name": "N. Santhi"
        },
        {
          "authorId": "3059326",
          "name": "Abdel-Hameed A. Badawy"
        },
        {
          "authorId": "1707687",
          "name": "S. Eidenbenz"
        }
      ],
      "abstract": "Recent years have seen the adoption of Machine Learning (ML) techniques to predict the performance of large-scale applications, mostly at a coarse level. In contrast, we propose to use ML techniques for performance prediction at a much finer granularity, namely at the Basic Block (BB) level, which are single entry, single exit code blocks that are used for analysis by the compilers to break down a large code into manageable pieces. Utilizing ML and BB analysis together can enable scalable hardware-software co-design beyond the current state of the art. In this work, we extrapolate the basic block execution counts of GPU applications and use it for predicting the performance for large input sizes from the counts of smaller input sizes.We trained a Poisson Neural Network (PNN) model using random input values as well as the lowest input values of the application to learn the relationship between inputs and basic block counts. Experimental results show that the model can accurately predict the basic block execution counts of 16 GPU benchmarks. We achieved an accuracy of 93.5% for extrapolating the basic block counts for large input sets when the model is trained using smaller input sets. Additionally, the model shows an accuracy of 97.7% for predicting basic block counts on random instances. In a significant case study, we applied the ML model to CUDA GPU benchmarks for performance prediction across a spectrum of applications, spanning linear algebra to machine learning benchmarks. We employed a diverse set of metrics for evaluation, including global memory requests, tensor cores\u2019 active cycles, and the active cycles of ALU and FMA units. The results from the case study demonstrate that the model is capable of predicting the performance of large datasets with high accuracy. For example, The average error rates for global and shared memory requests are 0.85% and 0.17%, respectively. Furthermore, to address the utilization of the main functional units in Ampere architecture GPUs, we calculated the active cycles for units like tensor cores, ALU, FMA, and FP64 units. Our predictions for the active cycles show an average error of 2.3% for the ALU and 10.66% for the FMA units, while the maximum observed error across all tested applications and units reaches 18.5%."
    },
    {
      "paperId": "b349173cd87c8e092feeff2d10bec818625c473c",
      "url": "https://www.semanticscholar.org/paper/b349173cd87c8e092feeff2d10bec818625c473c",
      "title": "Application of SSSC to the 330kV Nigerian transmission network for voltage control",
      "venue": "",
      "year": 2018,
      "openAccessPdf": {
        "url": "https://doi.org/10.4314/njt.v36i4.36",
        "status": "GOLD",
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.4314/NJT.V36I4.36?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.4314/NJT.V36I4.36, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "70139021",
          "name": "G. Adepoju"
        },
        {
          "authorId": "92514890",
          "name": "M. A. Sanusi"
        },
        {
          "authorId": "144322319",
          "name": "M. A. Tijani"
        },
        {
          "authorId": "67168362",
          "name": "OF Epartment"
        },
        {
          "authorId": "2098006941",
          "name": "Lectroni\u0307c"
        }
      ],
      "abstract": "Longitudinal power systems of Nigerian 330 kV transmission network have steady-state problems of congestion, voltage limit violation and high active power loss. Static Synchronous Series Compensator (SSSC) currently in use for solving problems in mesh power systems has not been applied to Nigerian 330 kV power network. This work involves the use of SSSC for solving problems associated with Nigerian 330 kV longitudinal power network using voltage magnitude as performance metrics. Steady state modeling of power system and SSSC modeling produced two sets of non-linear algebraic equations that were solved simultaneously using Newton-Raphson algorithm (NR) method and was implemented using MATLAB. Results of power flow analysis of Nigerian 330 kV transmission network without SSSC showed that, there was voltage limit violation of \u00b110% at bus 16 Gombe (0.8973p.u). However, the results with incorporation of SSSC showed that, the SSSC was effective in eliminating voltage limit violation, control bus voltage magnitude to specified value (bus 14 from 0.9462p.u. to 1.00p.u.) and reduced network active power loss by more than 5% of base case (93.87 MW). Therefore, SSSC is effective in solving steady-state problems of longitudinal power systems. Keywords: Longitudinal, Mesh, Newton-Raphson, SSSC modeling"
    },
    {
      "paperId": "20178ac9ba7184de48beb15feb202dca182e9824",
      "url": "https://www.semanticscholar.org/paper/20178ac9ba7184de48beb15feb202dca182e9824",
      "title": "Occamy: A 432-Core Dual-Chiplet Dual-HBM2E 768-DP-GFLOP/s RISC-V System for 8-to-64-bit Dense and Sparse Computing in 12-nm FinFET",
      "venue": "IEEE Journal of Solid-State Circuits",
      "year": 2025,
      "openAccessPdf": {
        "url": "https://www.research-collection.ethz.ch/bitstream/20.500.11850/714898/7/Occamy_JSSC.pdf",
        "status": "GREEN",
        "license": "other-oa",
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.07330, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2067713885",
          "name": "Paul Scheffler"
        },
        {
          "authorId": "143673573",
          "name": "Thomas Emanuel Benz"
        },
        {
          "authorId": "2303654593",
          "name": "Viviane Potocnik"
        },
        {
          "authorId": "2212188097",
          "name": "Tim Fischer"
        },
        {
          "authorId": "2294570264",
          "name": "Luca Colagrande"
        },
        {
          "authorId": "2332114366",
          "name": "Nils Wistoff"
        },
        {
          "authorId": "2184225256",
          "name": "Yichao Zhang"
        },
        {
          "authorId": "4075770",
          "name": "Luca Bertaccini"
        },
        {
          "authorId": "4338307",
          "name": "G. Ottavi"
        },
        {
          "authorId": "49285879",
          "name": "M. Eggimann"
        },
        {
          "authorId": "2059735702",
          "name": "Matheus A. Cavalcante"
        },
        {
          "authorId": "29437901",
          "name": "G. Paulin"
        },
        {
          "authorId": "30983085",
          "name": "Frank K. Gurkaynak"
        },
        {
          "authorId": "2253517096",
          "name": "Davide Rossi"
        },
        {
          "authorId": "2294566195",
          "name": "Luca Benini"
        }
      ],
      "abstract": "Machine learning (ML) and high-performance computing (HPC) applications increasingly combine dense and sparse memory access computations to maximize storage efficiency. However, existing central processing units (CPUs) and graphics processing units (GPUs) struggle to flexibly handle these heterogeneous workloads with consistently high compute efficiency. We present Occamy, a 432-core, 768-DP-GFLOP/s, dual-HBM2E, dual-chiplet RISC-V system with a latency-tolerant hierarchical interconnect and in-core streaming units (SUs) designed to accelerate dense and sparse FP8-to-FP64 ML and HPC workloads. We implement Occamy\u2019s compute chiplets in 12-nm FinFET and its passive interposer, Hedwig, in a 65-nm node. On dense linear algebra (LA), Occamy achieves a competitive floating-point unit (FPU) utilization of 89%. On stencil codes, Occamy reaches an FPU utilization of 83% and a technology-node-normalized compute density of 11.1 DP-GFLOP/s/mm2, leading state-of-the-art (SoA) processors by <inline-formula> <tex-math notation=\"LaTeX\">$1.7\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$1.2\\times $ </tex-math></inline-formula>, respectively. On sparse-dense LA, it achieves 42% FPU utilization and a normalized compute density of 5.95 DP-GFLOP/s/mm2, surpassing the SoA by <inline-formula> <tex-math notation=\"LaTeX\">$5.2\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$11\\times $ </tex-math></inline-formula>, respectively. On sparse\u2013sparse LA, Occamy reaches a throughput of up to 187 GCOMP/s at 17.4 GCOMP/s/W and a compute density of 3.63 GCOMP/s/mm2. Finally, we reach up to 75% and 54% FPU utilization on and dense (large language model) and graph-sparse (graph convolutional network) ML inference workloads. Occamy\u2019s register transfer level (RTL) description is freely available under a permissive open-source license."
    },
    {
      "paperId": "48e02f3dbe57e5051f12e86d3b8c8878f1c570b7",
      "url": "https://www.semanticscholar.org/paper/48e02f3dbe57e5051f12e86d3b8c8878f1c570b7",
      "title": "Synthesis of \u03b2-Cyclodextrin@gold Nanoparticles and Its Application on Colorimetric Assays for Ascorbic Acid and Salmonella Based on Peroxidase-like Activities",
      "venue": "Biosensors",
      "year": 2024,
      "openAccessPdf": {
        "url": "https://www.mdpi.com/2079-6374/14/4/169/pdf?version=1711868967",
        "status": "GOLD",
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11048340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2294575388",
          "name": "Xinyi Fan"
        },
        {
          "authorId": "2277914278",
          "name": "Yuexin Bao"
        },
        {
          "authorId": "2294390436",
          "name": "Yanhong Chen"
        },
        {
          "authorId": "2109072853",
          "name": "Xiaohong Wang"
        },
        {
          "authorId": "2265653380",
          "name": "S. On"
        },
        {
          "authorId": "2144547006",
          "name": "Jia Wang"
        }
      ],
      "abstract": "The peroxidase-like behaviors of gold nanoparticles (AuNPs) have the potential to the development of rapid and sensitive colorimetric assays for specific food ingredients and contaminants. Here, using NaBH4 as a reducing agent, AuNPs with a supramolecular macrocyclic compound \u03b2-cyclodextrin (\u03b2-CD) capped were synthesized under alkaline conditions. Monodispersal of \u03b2-CD@AuNPs possessed a reduction in diameter size and performed great peroxidase-like activities toward both substrates, H2O2 and TMB. In the presence of H2O2, the color change of TMB oxidization to oxTMB was well-achieved using \u03b2-CD@AuNPs as the catalyst, which was further employed to develop colorimetric assays for ascorbic acid, with a limit of detection as low as 0.2 \u03bcM in ddH2O. With the help of the host-guest interaction between \u03b2-CD and adamantane, AuNPs conjugated with nanobodies to exhibit peroxidase-like activities and specific recognition against Salmonella Typhimurium simultaneously. Based on this bifunctional bioprobe, a selective and sensitive one-step colorimetric assay for S. Typhimurium was developed with a linear detection from 8.3 \u00d7 104 to 2.6 \u00d7 108 CFU/mL and can be provided to spiked lettuce with acceptable recoveries of 97.31% to 103.29%. The results demonstrated that the excellent peroxidase-like behaviors of \u03b2-CD@AuNPs can be applied to develop a colorimetric sensing platform in the food industry."
    },
    {
      "paperId": "ed02e0527a5339638beddc733716d27c628b331b",
      "url": "https://www.semanticscholar.org/paper/ed02e0527a5339638beddc733716d27c628b331b",
      "title": "$C^*$-Algebraic Machine Learning: Moving in a New Direction",
      "venue": "",
      "year": 2024,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.02637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2254264945",
          "name": "Yuka Hashimoto"
        },
        {
          "authorId": "2254237002",
          "name": "Masahiro Ikeda"
        },
        {
          "authorId": "2282541096",
          "name": "Hachem Kadri"
        }
      ],
      "abstract": "Machine learning has a long collaborative tradition with several fields of mathematics, such as statistics, probability and linear algebra. We propose a new direction for machine learning research: $C^*$-algebraic ML $-$ a cross-fertilization between $C^*$-algebra and machine learning. The mathematical concept of $C^*$-algebra is a natural generalization of the space of complex numbers. It enables us to unify existing learning strategies, and construct a new framework for more diverse and information-rich data models. We explain why and how to use $C^*$-algebras in machine learning, and provide technical considerations that go into the design of $C^*$-algebraic learning models in the contexts of kernel methods and neural networks. Furthermore, we discuss open questions and challenges in $C^*$-algebraic ML and give our thoughts for future development and applications."
    },
    {
      "paperId": "ab8870977752e5930e309c4776e887741501703a",
      "url": "https://www.semanticscholar.org/paper/ab8870977752e5930e309c4776e887741501703a",
      "title": "The Mathematical Foundations of Machine Learning: A Comprehensive Review",
      "venue": "INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT",
      "year": 2024,
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55041/ijsrem40399?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55041/ijsrem40399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2338490389",
          "name": "Sanika Gonjari"
        },
        {
          "authorId": "2338488317",
          "name": "Sharwari Kshirsagar"
        },
        {
          "authorId": "2338431252",
          "name": "Priyanka Pawar"
        },
        {
          "authorId": "2338487570",
          "name": "Dr. G.J. Chhajed"
        }
      ],
      "abstract": "Machine learning (ML) is an interdisciplinary domain that utilizes mathematical concepts to create models for tasks such as prediction, classification, and data analysis. It employs linear algebra to manage extensive datasets and execute matrix computations, calculus for model optimization and error reduction, probability for uncertainty modeling and outcome prediction, and statistics for data distribution analysis and parameter estimation. These mathematical underpinnings empower machine learning to efficiently process information and develop intelligent systems. The applications of machine learning are widespread, impacting various sectors including cybersecurity, healthcare, smart city development, and agriculture. Significant challenges in the field include managing large volumes of data, enhancing model precision, and navigating ethical issues. The ongoing incorporation of sophisticated mathematical methods is further expanding the capabilities and reach of machine learning.\n\n \n\n \n\nKey Words: Machine learning, Linear algebra, Calculus, Probability, Statistics, Optimization and Data analysis\n\n"
    },
    {
      "paperId": "43169c29f2328d780883aad08d4ab8b19afcc9f4",
      "url": "https://www.semanticscholar.org/paper/43169c29f2328d780883aad08d4ab8b19afcc9f4",
      "title": "LAW: A Tool for Improved Productivity with High-Performance Linear Algebra Codes. Design and Applications",
      "venue": "",
      "year": 2007,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/0710.4896, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "3092939",
          "name": "T. Stitt"
        },
        {
          "authorId": "3032866",
          "name": "G. Kells"
        },
        {
          "authorId": "119565680",
          "name": "Jiri Vala Irish Centre for High-End Computing"
        },
        {
          "authorId": "2072622373",
          "name": "Ireland"
        },
        {
          "authorId": "102908846",
          "name": "N. U. Ireland"
        },
        {
          "authorId": "121827696",
          "name": "Maynooth"
        }
      ],
      "abstract": "LAPACK and ScaLAPACK are arguably the defacto standard libraries among the scientific community for solving linear algebra problems on sequential, shared-memory and distributed-memory architectures. While ease of use was a major design goal for the ScaLAPACK project; with respect to its predecessor LAPACK; it is still a non-trivial exercise to develop a new code or modify an existing LAPACK code to exploit processor grids, distributed-array descriptors and the associated distributed-memory ScaLAPACK/PBLAS routines. In this paper, we introduce what we believe will be an invaluable development tool for the scientific code developer, which exploits ad-hoc polymorphism, derived-types, optional arguments, overloaded operators and conditional compilation in Fortran 95, to provide wrappers to a subset of common linear algebra kernels. These wrappers are introduced to facilitate the abstraction of low-level details which are irrelevant to the science being performed, such as target platform and execution model. By exploiting this high-level library interface, only a single code source is required with mapping onto a diverse range of execution models performed at link-time with no user modification. We conclude with a case study whereby we describe application of the LAW library in the implementation of the well-known Chebyshev Matrix Exponentiation algorithm for Hermitian matrices."
    },
    {
      "paperId": "4bb11144db8fffc2a79942f172680cda44304364",
      "url": "https://www.semanticscholar.org/paper/4bb11144db8fffc2a79942f172680cda44304364",
      "title": "Designing of Nonlinear component of Block Cipher by a Moufang Loop and its Application in Image Encryption",
      "venue": "",
      "year": 2022,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "2130370166",
          "name": "Tasawwar Hussain"
        },
        {
          "authorId": "145196791",
          "name": "T. Shah"
        },
        {
          "authorId": "2157680508",
          "name": "Asif Ali"
        },
        {
          "authorId": "2137126210",
          "name": "R. Gul"
        },
        {
          "authorId": "2069562924",
          "name": "Muhammad Hussain"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "9824742a5d54a6643e570e25e19bc06d7e308f48",
      "url": "https://www.semanticscholar.org/paper/9824742a5d54a6643e570e25e19bc06d7e308f48",
      "title": "MacWilliams type identities on Sharma-Kaushik weights for linear codes over ring Zq",
      "venue": "International journal of information technology",
      "year": 2023,
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s41870-023-01368-7?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s41870-023-01368-7, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "46529481",
          "name": "M. Sridhar"
        },
        {
          "authorId": "2650737",
          "name": "Manohar Lal Kaushik"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "975c00a78fa19ca38d4607a9e887cd2ce8c5fadb",
      "url": "https://www.semanticscholar.org/paper/975c00a78fa19ca38d4607a9e887cd2ce8c5fadb",
      "title": "A multiscale preconditioner for crack evolution in porous microstructures: Accelerating phase\u2010field methods",
      "venue": "International Journal for Numerical Methods in Engineering",
      "year": 2024,
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1002/nme.7463?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/nme.7463, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2255560668",
          "name": "Kangan Li"
        },
        {
          "authorId": "2395291",
          "name": "Y. Mehmani"
        }
      ],
      "abstract": "Phase\u2010field methods are attractive for simulating the mechanical failure of geometrically complex porous microstructures described by 2D/3D x\u2010ray \u03bc$$ \\mu $$ CT images in subsurface (e.g., CO \u20092$$ {}_2 $$ storage) and manufacturing (e.g., Li\u2010ion battery) applications. They capture the nucleation, growth, and branching of fractures without prior knowledge of the propagation path or having to remesh the domain. Their drawback lies in the high computational cost for the typical domain sizes encountered in practice. We present a multiscale preconditioner that significantly accelerates the convergence of Krylov solvers in computing solutions of linear(ized) systems arising from the sequential discretization of the momentum and crack\u2010evolution equations in phase\u2010field methods. The preconditioner is an algebraic reformulation of a recent pore\u2010level multiscale method (PLMM) by the authors and consists of a global preconditioner MG$$ {\\mathrm{M}}_{\\mathrm{G}} $$ and a local smoother ML$$ {\\mathrm{M}}_{\\mathrm{L}} $$ . Together, MG$$ {\\mathrm{M}}_{\\mathrm{G}} $$ and ML$$ {\\mathrm{M}}_{\\mathrm{L}} $$ attenuate low\u2010 and high\u2010frequency errors simultaneously. The proposed MG$$ {\\mathrm{M}}_{\\mathrm{G}} $$ , used in the momentum equation only, is a simplification of a recent variant proposed by the authors that is much cheaper and easier to deploy in existing solvers. The smoother ML$$ {\\mathrm{M}}_{\\mathrm{L}} $$ , used in both the momentum and crack\u2010evolution equations, is built such that it is compatible with MG$$ {\\mathrm{M}}_{\\mathrm{G}} $$ and more robust and efficient than black\u2010box smoothers like ILU( k$$ k $$ ). We test MG$$ {\\mathrm{M}}_{\\mathrm{G}} $$ and ML$$ {\\mathrm{M}}_{\\mathrm{L}} $$ systematically for static\u2010 and evolving\u2010crack problems on complex 2D/3D porous microstructures, and show that they outperform existing algebraic multigrid solvers. We also probe different strategies for updating MG$$ {\\mathrm{M}}_{\\mathrm{G}} $$ as cracks evolve and show the associated cost can be minimized if MG$$ {\\mathrm{M}}_{\\mathrm{G}} $$ is updated adaptively and infrequently. Both MG$$ {\\mathrm{M}}_{\\mathrm{G}} $$ and ML$$ {\\mathrm{M}}_{\\mathrm{L}} $$ are scalable on parallel machines and can be implemented non\u2010intrusively in existing codes."
    },
    {
      "paperId": "431d243f71a375a0a4694f57888b9065d3cb37a9",
      "url": "https://www.semanticscholar.org/paper/431d243f71a375a0a4694f57888b9065d3cb37a9",
      "title": "Application of single-level, pointwise algebraic, and smoothed aggregation multigrid methods to direct numerical simulations of incompressible turbulent flows",
      "venue": "",
      "year": 2007,
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/S00791-006-0055-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/S00791-006-0055-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "152180159",
          "name": "G. Larson"
        },
        {
          "authorId": "34583609",
          "name": "D. Snyder"
        },
        {
          "authorId": "6807765",
          "name": "D. V. Abeele"
        },
        {
          "authorId": "1794004",
          "name": "T. Clees"
        }
      ],
      "abstract": null
    }
  ]
}